# -*- coding: utf-8 -*-
"""Comportamiento_Gatos.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1COeFg5TjRtAOdwsDLf07Y5TTyCdHSuFV
"""

# ==============================
# Bloque 1: Instalar de dependencias
# ==============================
!pip uninstall -y pinecone pinecone-client -q
!pip install --upgrade pip
!pip install pinecone-client==3.0.0
!pip install langchain-community
!pip install langchain openai datasets sentence-transformers pypdf flask-ngrok twilio
!pip install urllib3 pyngrok flask twilio requests deep-translator huggingface_hub transformers torch
!pip install huggingface_hub

# ==============================
# Bloque 2: Importar de librerías
# ==============================
import os
import requests
import urllib.request
from pyngrok import ngrok
from flask import Flask, request, Response
from twilio.twiml.messaging_response import MessagingResponse
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Pinecone as LangchainPinecone
from langchain.document_loaders import PyPDFLoader
from langchain_community.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain_community.llms import HuggingFaceHub
from pinecone import Pinecone, ServerlessSpec
import re
from deep_translator import GoogleTranslator
from transformers import pipeline

# ==============================
# Bloque 3: Variables - Claves de API
# ==============================
APIKEY_PINECONE = "pcsk_5CSBt8_FKwK9HAmaCXYqPxR9toNDeAfSMdt1zxsAmtCN93gSLESkX97GtUxAKxv8ZbVp5X"
PINECONE_ENVIRONMENT = "us-east-1"
APIKEY_OPENAI = "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"
APIKEY_NGROK = "2u9P8NYaHTAq2DOHUvcz22gTGBv_5tJk1W9YuqmbXC49WP8NF"
APIKEY_HUGGINGFACE = "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"
DEEPSEEK_API_KEY = "sk-65f8518837c04cd8808f6b0bc9f8d527"
# port = 5000  # Puerto

# os.environ["OPENAI_API_KEY"] = APIKEY_OPENAI
# os.environ["HUGGINGFACEHUB_API_TOKEN"] = "TU_APIKEY_HF"

# ==============================
# Bloque 4: Descarga de PDFs
# ==============================
def descargar_pdf(url, nombre_archivo):
    response = requests.get(url)
    with open(nombre_archivo, 'wb') as f:
        f.write(response.content)

pdf_urls = [
    ("https://comfenalcoquindio.com/wp-content/uploads/2021/11/el-lenguaje-de-los-gatos.pdf", "el-lenguaje-de-los-gatos.pdf"),
    ("https://aragohv.com/wp-content/uploads/2019/06/comportamientofelino.pdf", "comportamientofelino.pdf"),
    ("https://faada.org/docs/GuiaParaEntenderAlGato.pdf", "GuiaParaEntenderAlGato.pdf"),
    ("https://proassetspdlcom.cdnstatics2.com/usuaris/libros_contenido/arxius/54/53572_Al_gato_lo_que_es_del_gato.pdf", "53572_Al_gato_lo_que_es_del_gato.pdf"),
    ("https://catinfo.org/docs/Spanish.pdf", "Spanish.pdf")
]

for url, nombre in pdf_urls:
    descargar_pdf(url, nombre)

# ==============================
# Bloque 4.1: Cargar PDFs desde archivo local
# ==============================
local_pdfs = [
    # "ruta/a/tu_archivo1.pdf",
    # "ruta/a/tu_archivo2.pdf",
]

# Verificar cargar archivos en local_pdfs
for path in local_pdfs:
    if os.path.isfile(path):
        pdf_urls.append((None, path))
    else:
        print(f"Advertencia: No se encontró el archivo local '{path}'")

# ==============================
# Bloque 5: Cargar y procesar los PDFs
# ==============================
loaders = []
for url, nombre in pdf_urls:
    if url:
        loaders.append(PyPDFLoader(nombre))
    else:
        # Cargar desde ruta local
        loaders.append(PyPDFLoader(nombre))

docs = []
for loader in loaders:
    docs.extend(loader.load())

# ==============================
# Bloque 6: Creación embeddings con Hugging Face
# ==============================
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# ==============================
# Bloque 7: Configuración Pinecone e indexación de documentos
# ==============================
pc = Pinecone(api_key=APIKEY_PINECONE)
index_name = "gatos-rag"

if index_name not in pc.list_indexes().names():
    pc.create_index(
        name=index_name,
        dimension=384,
        metric="cosine",
        spec=ServerlessSpec(cloud="aws", region="us-east-1")
    )

index = pc.Index(index_name)

vectorstore = LangchainPinecone(
    index=index,
    embedding=embeddings,
    text_key="text"
)

vectorstore.add_documents(docs)

# ==============================
# Bloque 7.1: Insertar texto manual en Pinecone
# ==============================
def insertar_texto_manual(id, texto, metadatos=None):
    """
    Inserta un vector en Pinecone a partir de texto plano.
    - id: identificador único para el vector
    - texto: contenido a indexar
    - metadatos: diccionario de metadata opcional
    """
    # Generar embedding
    vector = embeddings.embed_documents([texto])[0]
    # Preparar metadatos incluyendo texto completo
    metadata_to_insert = metadatos.copy() if isinstance(metadatos, dict) else {}
    metadata_to_insert["text"] = texto
    # Insertar en índice
    index.upsert(vectors=[
        (id, vector, metadata_to_insert)
    ])
    print(f"Texto manual insertado con id={id} y metadatos={metadata_to_insert}")

# Ejemplo:
insertar_texto_manual("Comp_Gatos_3", "¿Hay diferencias de comportamiento y físicos de gatos entre países diferentes? Sí, existen diferencias de comportamiento y rasgos físicos entre gatos de diferentes países, aunque no tan marcadas como en perros debido a una menor diversidad de razas en gatos. Estas diferencias pueden estar influenciadas por factores genéticos, ambientales y de socialización. Diferencias de comportamiento: Razas, Sexo, Entorno, Socialización. Diferencias físicas Razas, Tamaño, Color, Caracteres sexuales secundarios. En resumen, los gatos son animales muy individualistas y su comportamiento y rasgos físicos pueden variar según una serie de factores, incluyendo la raza, el sexo, el entorno y la socialización. Aunque hay algunas diferencias generales entre gatos de diferentes países, cada gato tiene su propia personalidad y características únicas.")

# ==============================
# Bloque 8: Configuración de ngrok para exponer servidor
# ==============================
port = 5000  # Puerto configurado
ngrok.set_auth_token(APIKEY_NGROK)
public_url = ngrok.connect(port).public_url
print("Public URL para Twilio:", public_url)

# ==============================
# Bloque 8.1: Clase DeepSeek como LLM para LangChain
# ==============================
from langchain_core.language_models import LLM
from pydantic import BaseModel, Field
from typing import Optional, List
import requests

class DeepSeekLLM(LLM, BaseModel):
    api_key: str = Field(...)
    model: str = Field(default="deepseek-chat")

    @property
    def _llm_type(self) -> str:
        return "deepseek"

    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:
        print("Usando DeepSeek con LangChain (_call)")
        url = "https://api.deepseek.com/v1/chat/completions"
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        data = {
            "model": self.model,
            "messages": [
                # Mensaje
                {"role": "system",
                 "content": "Eres un asistente experto en comportamiento felino. Responde siempre en español de forma clara y concisa y por favor respuestas no mayor a 500 caracteres"},
                {"role": "user", "content": prompt}
            ],
            "temperature": 0.7,
            "max_tokens": 1024
        }

        try:
            response = requests.post(url, headers=headers, json=data)
            response.raise_for_status()
            return response.json()["choices"][0]["message"]["content"]
        except Exception as e:
            return f"Error al usar DeepSeek: {str(e)}"

# ==============================
# Bloque 8.2: Fragmentación de mensajes largos para Twilio
# ==============================
def split_chunks(text, size=400):
    """
    Divide el texto en trozos de tamaño máximo `size`.
    """
    return [text[i:i+size] for i in range(0, len(text), size)]

# ==============================
# Bloque 9: Configuración del modelo LLM y cadena RAG
# ==============================
from typing import Optional

USE_RAG = True # True activa RAG, False usa solo modelo LLM
MODEL_TYPE = "openai"  # Opciones: "openai", "deepseek"

PALABRAS_CLAVE = [
    "gato", "gatos", "felino", "felinos", "maullido", "ronroneo",
    "bigotes", "garra", "rabo", "michis", "mininos", "gaturros",
    "comida para gatos", "comportamiento felino", "salud de gatos",
    "rutinas de gatos", "comparación con perros"
]

def es_pregunta_valida(pregunta):
    return any(palabra in pregunta.lower() for palabra in PALABRAS_CLAVE)

# Función para seleccionar el modelo LLM
def get_llm(model_type):
    if model_type == "deepseek":
        return DeepSeekLLM(api_key=DEEPSEEK_API_KEY,
                  max_tokens=600,
                  temperature=0.7
        )
    elif model_type == "openai":
        return ChatOpenAI(
            model_name="gpt-3.5-turbo",
            temperature=0.7,
            max_tokens=600,
            openai_api_key=APIKEY_OPENAI
        )
    else:
        raise ValueError("Modelo no soportado: elige 'openai' o 'deepseek'")

# Inicializar el LLM según el modelo elegido
llm = get_llm(MODEL_TYPE)

# Inicializar la cadena RAG si está activada
if USE_RAG:
    # Limitar los documentos recuperados (k=2)
    retriever = vectorstore.as_retriever(search_kwargs={"k": 2})

    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        retriever=retriever,
        return_source_documents=False
    )

# ==============================
# Bloque 9.2: Función construir el prompt diferenciando RAG vs LLM
# ==============================
def build_prompt(pregunta: str, use_rag: bool) -> str:
    if use_rag:
        # Prefijo enfatizando uso de documentos (RAG)
        system_msg = (
            "Eres un asistente experto en comportamiento felino. "
            "Responde basándote en la información de los documentos proporcionados de forma clara y precisa."
            "Me respondes por favor sin ayuda del LLM ChatGPT, para saber si el contenido esta en la base de datos y no me esta respondiendo el LLM"
        )
    else:
        # Prefijo enfatizando creatividad y conocimiento general
        system_msg = (
            "Eres un asistente creativo y versátil en comportamiento felino. "
            "Responde con tu conocimiento general, de forma original y detallada."
        )
    # Construcción del prompt completo
    return f"{system_msg}\n\nPregunta: {pregunta}"

# ==============================
# Bloque 10/11 (Ultimo paso): Flask + Ngrok + Twilio - Ejecución y conversación en WhatsApp
# ==============================
from flask import Flask, request, Response
from twilio.twiml.messaging_response import MessagingResponse
from flask_ngrok import run_with_ngrok


app = Flask(__name__)
run_with_ngrok(app)    # Iniciar ngrok

MAX_WHATSAPP_LEN = 600  # antes dejé 1500
FINAL_COMMENT = "\n\nMáximo de 1500 caracteres y no des en la respuesta la cantidad de caracteres por favor"

def truncar_mensaje(mensaje, max_len=MAX_WHATSAPP_LEN):
    if len(mensaje) <= max_len:
        return mensaje
    corte = mensaje.rfind(' ', 0, max_len)
    if corte == -1:
        corte = max_len
    return mensaje[:corte].strip() + "..."

@app.route("/boot", methods=["POST"])
def bot():
    pregunta = request.values.get("Body", "").strip()
    print("Pregunta recibida:", pregunta)

    # 1) RAG o LLM sin RAG
    try:
        if USE_RAG:
            raw = qa_chain.invoke({"query": pregunta})
            respuesta = raw.get("result", "") if isinstance(raw, dict) else str(raw)
        else:
            respuesta = llm.predict(pregunta)
    except Exception as e:
        print("Error generando con RAG/LLM:", e)
        respuesta = ""

    # 2) DeepSeek sin RAG
    if MODEL_TYPE == "deepseek" and not respuesta.strip():
        print("DeepSeek no respondió, retry sin RAG...")
        try:
            respuesta = get_llm("deepseek").predict(pregunta)
        except Exception as e:
            print("Error en retry DeepSeek:", e)
            respuesta = "Lo siento, no pude procesar tu pregunta en este momento."

    # 3) Limpiar y truncar
    respuesta = respuesta.encode("utf-8").decode("utf-8")
    mensaje_final = truncar_mensaje(respuesta)

    print("Enviando a WhatsApp:", mensaje_final)
    tw_resp = MessagingResponse()
    tw_resp.message(mensaje_final)
    return Response(str(tw_resp), mimetype="application/xml")

if __name__ == "__main__":
    app.run()



#========================
# Extras - Adicional Costos Modelos
#========================
import os
import tiktoken
from openai import OpenAI

# 1. Variable API OpenAI
openai = OpenAI(api_key=APIKEY_OPENAI)

# 2. Función contar tokens
encoding = tiktoken.encoding_for_model("gpt-3.5-turbo")

def contar_tokens(texto: str) -> int:
    return len(encoding.encode(texto))

# 3. Prompt (parte RAG o sin RAG)
prompt = "¿Cuando mi gato se orina en mi casa, lo debo castigar?"
n_tokens_prompt = contar_tokens(prompt)

# 4. RAG: tokens embedding de la consulta
response_emb = openai.embeddings.create(
    model="text-embedding-ada-002",
    input=prompt
)
# Conteo tokens de embedding n_tokens_prompt sobre 1000000 por su costo:
costo_emb_query = (n_tokens_prompt / 1_000_000) * 0.40  # USD

# 5. Documentos RAG documentos indexados con ID
# 6. Se llama API
response = openai.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[{"role": "user", "content": prompt}],
    max_tokens=800
)

# 7. Uso de tokens de la respuesta usando atributos del objeto
tokens_input = response.usage.prompt_tokens       # número de tokens del prompt
tokens_output = response.usage.completion_tokens  # número de tokens generados en la respuesta

# 8. Calculo de costos
costo_input = (tokens_input / 1_000_000) * 0.50    # $0.50 por 1M tokens
costo_output = (tokens_output / 1_000_000) * 1.50  # $1.50 por 1M tokens
costo_completion = costo_input + costo_output

# 9. Sumatoria de costos (embeddings + completion)
costo_total = costo_emb_query + costo_completion

print(f"Tokens prompt: {tokens_input}, tokens respuesta: {tokens_output}")
print(f"Costo embedding consulta: ${costo_emb_query:.6f} USD")
print(f"Costo completion: ${costo_completion:.6f} USD")
print(f"→ Costo total de esta llamada: ${costo_total:.6f} USD")

#========================
# Extras - Adicional Costos Modelos
#========================
import tiktoken

encoding = tiktoken.encoding_for_model("gpt-3.5-turbo")

def contar_tokens(texto: str) -> int:
    return len(encoding.encode(texto))

# Ejemplo
prompt = "¿Cuando mi gato se orina en mi casa, lo debo castigar?"
tokens_prompt = contar_tokens(prompt)
print(f"Tokens usados en el prompt: {tokens_prompt}")

MAX_DEEPSEEK_LEN = 800  # límite respuestas WhatsApp DeepSeek

def truncar_deepseek_si_necesario(texto):
    if MODEL_TYPE == "deepseek" and len(texto) > MAX_DEEPSEEK_LEN:
        corte = texto.rfind(' ', 0, MAX_DEEPSEEK_LEN)
        if corte == -1:
            corte = MAX_DEEPSEEK_LEN
        return texto[:corte].strip() + "..."
    return texto

#==========================
# Extras - Costos Token x DeepSeek
#==========================
import os
import tiktoken
import requests
from openai import OpenAI

# 1. Configuración de claves de API
#   - APIKEY_OPENAI: clave OpenAI
#   - APIKEY_DEEPSEEK: clave DeepSeek

openai = OpenAI(api_key=APIKEY_OPENAI)
DEEPSEEK_API_KEY = DEEPSEEK_API_KEY
DEEPSEEK_ENDPOINT = "https://api.deepseek.com/v1/chat/completions"

# 2. Funciones contar tokens con tiktoken-ambos LLMs
encoding = tiktoken.encoding_for_model("gpt-3.5-turbo")

def contar_tokens(texto: str) -> int:
    """
    Retorna el número de tokens de un texto plano usando tiktoken.encode().
    """
    return len(encoding.encode(texto))

# 3. Función auxiliar calcular costo según modelo

def costo_openai(prompt_tokens: int, completion_tokens: int, costo_emb: float = 0.0) -> float:

    # Precios (junio 2025): $0.50 por 1M input, $1.50 por 1M output
    costo_input  = (prompt_tokens     / 1_000_000) * 0.50
    costo_output = (completion_tokens / 1_000_000) * 1.50
    return costo_input + costo_output + costo_emb  # incluir costo de embedding(si existe)

def costo_deepseek(tokens_input: int, tokens_output: int, costo_emb: float = 0.0) -> float:

    costo_input  = (tokens_input  / 1_000_000) * 0.27  # :contentReference[oaicite:2]{index=2}
    costo_output = (tokens_output / 1_000_000) * 1.10  # :contentReference[oaicite:3]{index=3}
    return costo_input + costo_output + costo_emb

# 4. Función para DeepSeek

def llamar_deepseek_chat(prompt: str) -> (str, int, int):

    headers = {
        "Authorization": f"Bearer {DEEPSEEK_API_KEY}",
        "Content-Type": "application/json"
    }
    payload = {
        "model": "deepseek-chat",
        "messages": [{"role": "user", "content": prompt}],
        "max_tokens": 800
    }
    resp = requests.post(DEEPSEEK_ENDPOINT, json=payload, headers=headers)
    resp.raise_for_status()
    data = resp.json()

    # Respuesta
    respuesta = data["choices"][0]["message"]["content"]

    # Conteo tokens manualmente con tiktoken:
    tokens_in  = contar_tokens(prompt)
    tokens_out = contar_tokens(respuesta)

    return respuesta, tokens_in, tokens_out

# 5. Función para OpenAI

def llamar_openai_chat(prompt: str) -> (str, int, int):

    response = openai.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": prompt}],
        max_tokens=800
    )
    respuesta    = response.choices[0].message.content
    tokens_input  = response.usage.prompt_tokens
    tokens_output = response.usage.completion_tokens
    return respuesta, tokens_input, tokens_output

# 6. Ejecución principal: LLM (opcional RAG) calculo todo
if __name__ == "__main__":
    # A. Parámetros generales
    prompt = "¿Cuando mi gato se orina en mi casa, lo debo castigar?"
    uso_rag = True  # False para no usar embeddings RAG

    # B. Calcular costo de embeddings con RAG
    costo_emb_query = 0.0
    if uso_rag:
        # Convertir prompt embedding OpenAI
        response_emb = openai.embeddings.create(
            model="text-embedding-ada-002",
            input=prompt
        )
        # usar contar_tokens() para estimar n_tokens_prompt:
        n_tokens_prompt = contar_tokens(prompt)
        costo_emb_query = (n_tokens_prompt / 1_000_000) * 0.40  # $0.40 por 1M tokens

    # C. Selección modelo
    # "openai" o "deepseek"
    modelo_elegido = "openai"

    if modelo_elegido == "openai":
        # 1) OpenAI
        respuesta, tokens_in, tokens_out = llamar_openai_chat(prompt)

        # 2) Cálculo costo total
        costo_total = costo_openai(tokens_in, tokens_out, costo_emb_query)

    elif modelo_elegido == "deepseek":
        # 1) DeepSeek
        respuesta, tokens_in, tokens_out = llamar_deepseek_chat(prompt)

        # 2) Cálculo de costo total
        costo_total = costo_deepseek(tokens_in, tokens_out, costo_emb_query)

    else:
        raise ValueError("Elige modelo 'openai' o 'deepseek' en la variable 'modelo_elegido'.")

    # ── D. Impresión de resultados
    print("──── RESULTADOS ─────────────────────────────────────────")
    print(f"Modelo usado: {modelo_elegido}")
    print(f"Prompt:\n{prompt}\n")
    print(f"Respuesta generada:\n{respuesta}\n")
    print(f"Tokens (input):  {tokens_in}")
    print(f"Tokens (output): {tokens_out}")
    if uso_rag:
        print(f"Costo embedding consulta (RAG): ${costo_emb_query:.6f} USD")
    print(f"→ Costo total generación: ${costo_total:.6f} USD")

from google.colab import files
files.upload()  # EncuestaModelos.xlsx

# Extras - 1) Subir archivo y cargarlo en DataFrame
from google.colab import files
import io
import pandas as pd

uploaded = files.upload()                         # Subir “EncuestaModelos.xlsx”
filename = list(uploaded.keys())[0]
df = pd.read_excel(io.BytesIO(uploaded[filename]), sheet_name=0)

# 2) Limpiar filas vacías e identifica las secciones por el encabezado “Modo”
df = df.dropna(how='all').reset_index(drop=True)
inicio = df.index[df.iloc[:,0] == 'Modo'].tolist()  # Busca “Modo” en la primera columna

# 3) Extraer las tres tablas en una lista de DataFrames
tablas = []
for i, idx in enumerate(inicio):
    end = inicio[i+1] if i+1 < len(inicio) else len(df)
    tabla = df.iloc[idx:end].copy()
    tabla.columns = tabla.iloc[0]    # convierte la fila de encabezado en nombres de columna
    tabla = tabla.drop(idx).reset_index(drop=True)
    # Convertir columnas numéricas
    tabla['Cantidad'] = tabla['Cantidad'].astype(int)
    #
    tabla['%'] = (
        tabla['%']
        .astype(str)
        .str.replace('%','')
        .str.replace(',','.')
        .astype(float)
    )
    tablas.append(tabla)

# 4) Mostrar las tablas
for i, t in enumerate(tablas, 1):
    print(f"\n--- Tabla {i} ---")
    display(t)

# 5) Graficar cada tabla con matplotlib
import matplotlib.pyplot as plt

for i, t in enumerate(tablas, 1):
    plt.figure()
    plt.bar(t['Modo'], t['Cantidad'])
    plt.title(f"Distribución de Cantidad – Tabla {i}")
    plt.xlabel("Modo")
    plt.ylabel("Cantidad")
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.show()

# Extras - Graficos
import pandas as pd
import matplotlib.pyplot as plt

# Datos de resultados de la encuesta
data = {
    "Modelo": [
        "DeepSeek sin RAG",
        "ChatGPT sin RAG",
        "ChatGPT con RAG",
        "DeepSeek con RAG",
        "Empate"
    ],
    "Cantidad": [125, 107, 77, 54, 37]
}

df = pd.DataFrame(data)
df['Porcentaje'] = df['Cantidad'] / df['Cantidad'].sum() * 100

# Mostrar el DataFrame actualizado
display(df)

# Graficar barras: Cantidad por Modelo y agregar porcentaje
plt.figure()
bars = plt.bar(df["Modelo"], df["Cantidad"])
plt.xlabel("Modelo")
plt.ylabel("Cantidad de preferencias")
plt.title("Preferencias de usuarios por configuración de modelo")
plt.xticks(rotation=45, ha='right')

# Añadir etiquetas de porcentaje encima de cada barra
for bar, pct in zip(bars, df['Porcentaje']):
    height = bar.get_height()
    plt.text(
        bar.get_x() + bar.get_width() / 2,
        height + 3,  # altura ligeramente por encima de la barra
        f"{pct:.1f}%",
        ha='center',
        va='bottom'
    )

plt.tight_layout()
plt.show()

# @title Modelo vs Cantidad

from matplotlib import pyplot as plt
import seaborn as sns
figsize = (12, 1.2 * len(df['Modelo'].unique()))
plt.figure(figsize=figsize)
sns.violinplot(df, x='Cantidad', y='Modelo', inner='stick', palette='Dark2')
sns.despine(top=True, right=True, bottom=True, left=True)

# Extras - Graficos
from google.colab import files
import io
import pandas as pd
import matplotlib.pyplot as plt

# 1) Subir y leer el Excel
uploaded = files.upload()
filename = list(uploaded.keys())[0]
df = pd.read_excel(io.BytesIO(uploaded[filename]), sheet_name=0)

# 2) Limpiar y extraer tablas (Modo, Pregunta, Etiqueta)
df = df.dropna(how='all').reset_index(drop=True)
inicios = df.index[df.iloc[:,0] == 'Modo'].tolist()
tablas = []
for i, start in enumerate(inicios):
    end = inicios[i+1] if i+1 < len(inicios) else len(df)
    tbl = df.iloc[start:end].copy()
    tbl.columns = tbl.iloc[0]
    tbl = tbl.drop(start).reset_index(drop=True)
    # Asegurarse de tipos
    tbl['Cantidad'] = tbl['Cantidad'].astype(int)
    tablas.append(tbl)

# 3) Gráfica: Cantidad por Pregunta (tabla 2)
df_preg = tablas[1]
plt.figure()
plt.bar(df_preg['Pregunta'], df_preg['Cantidad'])
plt.xlabel("Pregunta")
plt.ylabel("Cantidad de selecciones")
plt.title("Selecciones por Pregunta")
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

# 4) Gráfica: Cantidad por Etiqueta (tabla 3)
df_etq = tablas[2]
plt.figure()
plt.bar(df_etq['Etiqueta'], df_etq['Cantidad'])
plt.xlabel("Etiqueta")
plt.ylabel("Cantidad de ocurrencias")
plt.title("Distribución de Etiquetas")
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

# Extras - Graficos
import pandas as pd
import matplotlib.pyplot as plt

# Datos de resultados encuesta primera gráfica
data = {
    "Modelo": [
        "DeepSeek sin RAG",
        "ChatGPT sin RAG",
        "ChatGPT con RAG",
        "DeepSeek con RAG",
        "Empate"
    ],
    "Cantidad": [125, 107, 77, 54, 37]
}

df = pd.DataFrame(data)

# Mostrar el DataFrame
display(df)

# Gráfico de torta: proporción de preferencias por modelo
plt.figure()
plt.pie(df["Cantidad"], labels=df["Modelo"], autopct="%1.1f%%", startangle=90)
plt.title("Distribución de preferencias de usuarios por modelo")
plt.axis('equal')  # Torta como círculo
plt.show()

# Extras - Graficos
import pandas as pd
import matplotlib.pyplot as plt

# 1) Cargar hoja de resultados
file_path = '/mnt/data/EncuestaModelos.xlsx'
df = pd.read_excel(file_path, sheet_name=0)

# 2) Limpiar filas vacías
df = df.dropna(how='all').reset_index(drop=True)

# 3) Agrupar por Pregunta y cuenta
conteo_preg = df.groupby('Pregunta').size().reset_index(name='Recuento')
# 4) Graficar Pregunta vs Recuento
plt.figure()
plt.bar(conteo_preg['Pregunta'], conteo_preg['Recuento'])
plt.xlabel('Pregunta')
plt.ylabel('Recuento')
plt.title('Cantidad de selecciones por Pregunta')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

# 5) Agrupar por Etiqueta y cuenta
conteo_etq = df.groupby('Etiqueta').size().reset_index(name='Recuento')
# 6) Graficar Etiqueta vs Recuento
plt.figure()
plt.bar(conteo_etq['Etiqueta'], conteo_etq['Recuento'])
plt.xlabel('Etiqueta')
plt.ylabel('Recuento')
plt.title('Cantidad de ocurrencias por Etiqueta')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

# Extras - Metodo IC Wilson - Comparación entre modelos

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
from statsmodels.stats.proportion import proportion_confint

# 1. Cargar de archivo CSV (subido a /mnt/data/EncuestaModelos.csv)
df = pd.read_csv('/content/EncuestaModelos.csv', sep=';', encoding='latin-1')

# 2. Mapear 'Valor' a 'Score' ordinal centrado en cero
map_score = {0: 2, 1: 1, 2: 0, 3: -1, 4: -2}
df['Score'] = df['Valor'].map(map_score)
df['Victory'] = df['Score'] > 0

# 3. Estadísticas descriptivas por modelo
grouped = df.groupby('Modelo')
stats_df = grouped['Score'].agg(['mean', 'std', 'count']).reset_index()
stats_df['se'] = stats_df['std'] / np.sqrt(stats_df['count'])
t_val = stats.t.ppf(0.975, df=stats_df['count'] - 1)
stats_df['ci_low']  = stats_df['mean'] - t_val * stats_df['se']
stats_df['ci_high'] = stats_df['mean'] + t_val * stats_df['se']

# 4. Proporción de victorias con IC de Wilson
prop = grouped['Victory'].agg(['sum', 'count']).reset_index()
prop['prop'] = prop['sum'] / prop['count']
ci_low, ci_high = proportion_confint(prop['sum'], prop['count'], method='wilson')
prop['ci_low']  = ci_low
prop['ci_high'] = ci_high

# 5. Gráfico 1: Score medio con IC 95%
plt.figure()
plt.bar(stats_df['Modelo'], stats_df['mean'],
        yerr=[stats_df['mean']-stats_df['ci_low'], stats_df['ci_high']-stats_df['mean']],
        capsize=5)
plt.ylabel('Score medio')
plt.xticks(rotation=45, ha='right')
plt.title('Score medio por modelo con IC 95%')
plt.tight_layout()
plt.show()

# 6. Gráfico 2: Tasa de victorias con IC 95%
plt.figure()
plt.bar(prop['Modelo'], prop['prop'],
        yerr=[prop['prop']-prop['ci_low'], prop['ci_high']-prop['prop']],
        capsize=5)
plt.ylabel('Proporción de victorias')
plt.xticks(rotation=45, ha='right')
plt.title('Tasa de victorias por modelo con IC 95%')
plt.tight_layout()
plt.show()